{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMOgkoeKe7XSqJ8Wch0ou8u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macroact-lab/robotics-book/blob/main/6_2_grid_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFD3O8fGXEid"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from matplotlib.colors import ListedColormap\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorldEnv:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.actions = [\"up\", \"right\", \"down\", \"left\"]\n",
        "        self.grid = np.zeros((size, size))\n",
        "\n",
        "        # 장애물(1)과 목적지(2) 배치\n",
        "        self.grid[1, 1] = 1\n",
        "        self.grid[2, 1] = 1\n",
        "        self.grid[1, 3] = 1\n",
        "        self.grid[3, 3] = 1\n",
        "        self.grid[size-1, size-1] = 2\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"환경 초기화: 로봇을 시작점(0,0)으로 보낸다.\"\"\"\n",
        "        self.agent_pos = [0, 0]\n",
        "        # (관측값, 추가정보)를 반환하는 표준 형식을 따른다.\n",
        "        return tuple(self.agent_pos), {}\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        \"\"\"에이전트의 행동을 받아 상태를 변화시킨다.\"\"\"\n",
        "        action = self.actions[action_idx]\n",
        "        old_pos = self.agent_pos.copy()\n",
        "\n",
        "        # 1. 위치 이동 계산\n",
        "        if action == \"up\" and self.agent_pos[0] > 0:\n",
        "            self.agent_pos[0] -= 1\n",
        "        elif action == \"right\" and self.agent_pos[1] < self.size - 1:\n",
        "            self.agent_pos[1] += 1\n",
        "        elif action == \"down\" and self.agent_pos[0] < self.size - 1:\n",
        "            self.agent_pos[0] += 1\n",
        "        elif action == \"left\" and self.agent_pos[1] > 0:\n",
        "            self.agent_pos[1] -= 1\n",
        "\n",
        "        # 2. 결과(보상/종료여부) 판정\n",
        "        terminated = False\n",
        "        if self.grid[self.agent_pos[0], self.agent_pos[1]] == 1: # 장애물 충돌\n",
        "            self.agent_pos = old_pos # 뒤로가기\n",
        "            reward = -1\n",
        "        elif self.grid[self.agent_pos[0], self.agent_pos[1]] == 2: # 목적지\n",
        "            reward = 10\n",
        "            terminated = True\n",
        "        else: # 일반 이동 (최단 경로 유도를 위해 작은 페널티)\n",
        "            reward = -0.1\n",
        "\n",
        "        # 표준 반환: 다음상태, 보상, 종료, 중단(여기선 사용X), 정보\n",
        "        return tuple(self.agent_pos), reward, terminated, False, {}\n",
        "\n",
        "    def render(self, q_table=None):\n",
        "        \"\"\"로봇의 위치와 지능(Q-table)을 화면에 그린다.\"\"\"\n",
        "        vis_grid = self.grid.copy()\n",
        "        vis_grid[self.agent_pos[0], self.agent_pos[1]] = 3\n",
        "        colors = ['white', 'black', 'green', 'red']\n",
        "        cmap = ListedColormap(colors)\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        sns.heatmap(vis_grid, cmap=cmap, vmin=0, vmax=3, cbar=False, linewidths=1, linecolor='black', square=True)\n",
        "        plt.title(\"Grid World Navigation\")\n",
        "        plt.show(block=False)\n",
        "        plt.pause(0.1)\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "X127u-beXJ6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, state_size, action_size, alpha=0.1, gamma=0.99, epsilon=1.0):\n",
        "        self.alpha = alpha # 학습률 (새로운 정보를 얼마나 받아들일까?)\n",
        "        self.gamma = gamma # 할인율 (미래의 보상을 얼마나 중요시할까?)\n",
        "        self.epsilon = epsilon # 탐험률 (새로운 길을 가볼까, 아는 길로 갈까?)\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.01\n",
        "\n",
        "        # Q-Table 초기화: 모든 칸에서 각 행동의 가치를 0으로 시작한다.\n",
        "        self.q_table = {}\n",
        "        for i in range(state_size):\n",
        "            for j in range(state_size):\n",
        "                self.q_table[(i, j)] = np.zeros(action_size)\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        \"\"\"Epsilon-Greedy 전략으로 다음 행동을 결정한다.\"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, 3) # 랜덤 탐험\n",
        "        else:\n",
        "            return np.argmax(self.q_table[obs]) # 아는 길 중 최선 선택\n",
        "\n",
        "    def update(self, obs, action, reward, next_obs):\n",
        "        \"\"\"Q-Learning 공식을 사용하여 지식을 업데이트합니다.\"\"\"\n",
        "        old_value = self.q_table[obs][action]\n",
        "        next_max = np.max(self.q_table[next_obs])\n",
        "\n",
        "        # 공식: Q = Q + alpha * (보상 + 미래가치 - 현재가치)\n",
        "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "        self.q_table[obs][action] = new_value\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"학습이 진행될수록 탐험을 줄이고 아는 길로 가게 한다.\"\"\"\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
      ],
      "metadata": {
        "id": "zCkYmyaVXOj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 준비 단계\n",
        "env = GridWorldEnv(size=5)\n",
        "agent = QLearningAgent(state_size=5, action_size=4)\n",
        "n_episodes = 300\n",
        "\n",
        "# 2. 학습 루프 시작\n",
        "for episode in range(n_episodes):\n",
        "    # 환경 초기화\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # [판단] 현재 상태에서 행동 선택\n",
        "        action = agent.get_action(obs)\n",
        "\n",
        "        # [실행] 행동을 취하고 피드백(보상 등) 받기\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # [학습] 에이전트의 지식 업데이트\n",
        "        agent.update(obs, action, reward, next_obs)\n",
        "\n",
        "        # 다음 단계 준비\n",
        "        total_reward += reward\n",
        "        obs = next_obs\n",
        "        done = terminated or truncated\n",
        "\n",
        "    # 한 게임이 끝날 때마다 탐험률 줄이기\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "    if (episode + 1) % 50 == 0:\n",
        "        print(f\"에피소드: {episode+1}, 보상: {total_reward:.1f}, 탐험률: {agent.epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n--- 학습 완료! 이제 로봇이 길을 찾습니다. ---\")\n",
        "\n",
        "# 3. 테스트 (배운 실력 확인하기)\n",
        "obs, info = env.reset()\n",
        "env.render(agent.q_table) # 학습된 지능 시각화"
      ],
      "metadata": {
        "id": "BozRk9J0XSxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}